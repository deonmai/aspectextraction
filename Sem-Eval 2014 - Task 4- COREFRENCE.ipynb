{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read articles and get idea of what to do\n",
    "2. Dataset -> pre process and clean up. Remove stop words, lemmatize, \n",
    "3. Develop my own method to extract aspect terms.\n",
    "4. evaluate, compare to other studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neuralcoref --no-binary neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = xml.dom.minidom.parse(\"test_datasets/restaurants-trial.xml\")\n",
    "root = ET.parse(\"test_datasets/restaurants-trial.xml\").getroot()\n",
    "dataset = ET.parse(\"test_datasets/restaurants-trial.xml\").getroot().findall('sentence')\n",
    "\n",
    "actual_dataset = ET.parse(\"test_datasets/restaurants-trial.xml\").getroot().findall('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# print(dataset.firstChild.tagName)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aspect:\n",
    "    '''Aspect objects contain the term (e.g., battery life) and polarity (i.e., pos, neg, neu, conflict) of an aspect.'''\n",
    "\n",
    "    def __init__(self, term, polarity, offsets):\n",
    "        self.term = term\n",
    "        self.polarity = polarity\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def create(self, element):\n",
    "        self.term = element.attrib['term']\n",
    "        self.polarity = element.attrib['polarity']\n",
    "        self.offsets = {'from': str(element.attrib['from']), 'to': str(element.attrib['to'])}\n",
    "        return self\n",
    "\n",
    "    def update(self, term='', polarity=''):\n",
    "        self.term = term\n",
    "        self.polarity = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(filename):\n",
    "    elements = filename\n",
    "    aspects = []\n",
    "    text = []\n",
    "    for e in elements:\n",
    "        sent_id = e.attrib['id']\n",
    "        for f in e.findall('text'):\n",
    "            text.append((f, sent_id))\n",
    "        for eterms in e.findall('aspectTerms'):\n",
    "            if eterms is not None:\n",
    "                for a in eterms.findall('aspectTerm'):\n",
    "                    aspects.append((Aspect('', '', []).create(a), sent_id))\n",
    "    return text, aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "id:  813\n",
      "And really large portions.\n",
      "appetizers\n",
      "salads\n",
      "steak\n",
      "pasta\n"
     ]
    }
   ],
   "source": [
    "text = validate(dataset)[0]\n",
    "print(len(text))\n",
    "sent_id = text[0][1]\n",
    "print(\"id: \", sent_id)\n",
    "\n",
    "print (text[1][0].text)\n",
    "\n",
    "aspects = validate(dataset)[1]\n",
    "for i in range(len(aspects)):\n",
    "    if(aspects[i][1] == sent_id):\n",
    "        print(aspects[i][0].term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!\n"
     ]
    }
   ],
   "source": [
    "for sentence in root.findall(\"./sentence/[@id = '813']\"):\n",
    "    print (sentence.find(\"text\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEXICON - subjectivity clues Bing Liu\n",
    "SubClue = namedtuple('SubClue', ['rel', 'pri_pol', 'stemmed'])\n",
    "class SubjectivityClues(object):\n",
    "    # map POS tags in subjectivty clues to spacy tags:\n",
    "    _pos_map = {\n",
    "        'noun': 'NOUN',\n",
    "        'verb': 'VERB',\n",
    "        'adj': 'ADJ',\n",
    "        'adverb': 'ADV',\n",
    "        'anypos': 'ANYPOS',\n",
    "        }\n",
    "\n",
    "    def __init__(self, sc_path=\n",
    "            'lexicons/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.tff'):\n",
    "\n",
    "        # load subjectivity clues\n",
    "        with open(sc_path, 'r') as fo:\n",
    "            sub_clues = [c.strip().replace('type=', 'rel=').split()\n",
    "                         for c in fo]\n",
    "        sub_clues = [dict([d.split('=') for d in sc if len(d.split('=')) == 2])\n",
    "                     for sc in sub_clues]\n",
    "\n",
    "        # convert to dict keyed by (word, part-of-speech) and exclude neutral\n",
    "        # polarity clues:\n",
    "        # ISSUE: if stemmed for matches non-stemmed will overwrite (may not\n",
    "        # have impact as polarity and type remain the same\n",
    "        self.lex = {(s['word1'], self._pos_map[s['pos1']]): SubClue(s['rel'],\n",
    "                     s['priorpolarity'],\n",
    "                     True if s['stemmed1'] == 'y' else False)\n",
    "                    for s in sub_clues if s['priorpolarity']}\n",
    "        # sclues_pos and sclues_any are just sets containing all (word, pos)\n",
    "        # tuples with a proper POS tag and 'ANYPOS' respectively\n",
    "        self.sclues_pos = set([(w, t) for w, t in self.lex.keys()\n",
    "                               if t != 'ANYPOS'])\n",
    "        self.sclues_any = set(self.lex.keys()).difference(self.sclues_pos)\n",
    "\n",
    "    def lookup(self, token):\n",
    "        \"\"\"\n",
    "        Returns a `SubClue` named tuple for `token` if either `(token.norm_,\n",
    "        token.pos_)` or `(token.norm_, 'ANYPOS') is in self.lex. Returns an\n",
    "        empty tuple if no match is found.\n",
    "        \"\"\"\n",
    "        tnp = (token.norm_, token.pos_)\n",
    "        tna = (token.norm_, 'ANYPOS')\n",
    "        if tnp in self.lex or tna in self.lex:\n",
    "            key = tnp if tnp in self.lex else tna\n",
    "            return self.lex[key]\n",
    "        else:\n",
    "            return tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjclues = SubjectivityClues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "# from spacy.util import filter_spans\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import neuralcoref\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "pronoun_list=['he','she','it','they','them','him','her','his','hers','its','we','us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1055f78d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get verb chunks\n",
    "def get_verb_chunks(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern = [{'POS':'VERB', \"OP\":\"*\"},\n",
    "              {'POS':'ADV', \"OP\":\"*\"},\n",
    "              {'POS':'PART', \"OP\":\"*\"},\n",
    "              {'POS':'VERB', \"OP\":\"+\"},\n",
    "              {'POS':'PART', \"OP\":\"*\"}]\n",
    "    \n",
    "    matcher.add(\"verb_clause\", None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "    \n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "    \n",
    "    return(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priced\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The food is great and reasonably priced.\")\n",
    "for chunk in get_verb_chunks(doc):\n",
    "    print (chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun_chunks from spacy\n",
    "\n",
    "from spacy.symbols import NOUN, PROPN, PRON\n",
    "\n",
    "def noun_chunks(obj):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    doc = obj.doc  # Ensure works on both Doc and Span.\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    seen = set()\n",
    "    for i, word in enumerate(obj):\n",
    "#         print(\"i\", i)\n",
    "#         print(\"word\", word)\n",
    "        if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.i in seen:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            if any(w.i in seen for w in word.subtree):\n",
    "                continue\n",
    "            seen.update(j for j in range(word.left_edge.i, word.i + 1))\n",
    "            yield word.left_edge.i, word.i + 1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                if any(w.i in seen for w in word.subtree):\n",
    "                    continue\n",
    "                seen.update(j for j in range(word.left_edge.i, word.i + 1))\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "\n",
    "\n",
    "SYNTAX_ITERATORS = {\"noun_chunks\": noun_chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our agreed favorite \n",
      "the orrechiete \n",
      "sausage\n",
      "chicken\n",
      "the waiters \n",
      "the dish \n",
      "half\n",
      "you\n",
      "both meats \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Our agreed favorite is the orrechiete with sausage and chicken (usually the waiters are kind enough to split the dish in half so you get to sample both meats).\")\n",
    "\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text)\n",
    "\n",
    "for chunk in noun_chunks(doc):\n",
    "    if(chunk[1] - chunk[0] == 1):\n",
    "        print(doc[chunk[0]])\n",
    "    else:\n",
    "        for i in range(chunk[1]-chunk[0]):\n",
    "            print(doc[chunk[0]+i], end = \" \")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even \t ADV \t SubClue(rel='weaksubj', pri_pol='positive', stemmed=False)\n",
      "though \t ADP \t SubClue(rel='strongsubj', pri_pol='neutral', stemmed=False)\n",
      "good \t ADJ \t SubClue(rel='weaksubj', pri_pol='positive', stemmed=False)\n",
      "too \t ADV \t SubClue(rel='weaksubj', pri_pol='negative', stemmed=True)\n",
      "high \t ADJ \t SubClue(rel='weaksubj', pri_pol='neutral', stemmed=False)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Even though its good seafood, the prices are too high.\")\n",
    "for token in doc:\n",
    "    if(subjclues.lookup(token)):\n",
    "        print(token.text, '\\t', token.pos_, '\\t', subjclues.lookup(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last', 'hereafter', 'get', 'due', 'whereby', 'give', 'with', \"'s\", 'besides', 'might', 'amount', 'thereafter', 'was', 'part', 'fifteen', 'in', 'side', 'latterly', 'one', 'less', 'any', 'every', 'please', 'regarding', 'keep', 'though', 'across', 'it', 'here', 'becoming', 'together', 'take', 'before', 'to', 'call', 'and', 'us', 'eight', 'everything', 'someone', 'her', 'will', 'somehow', 'two', 'via', 'may', 'cannot', 'do', 'least', 'back', 'can', 'too', \"'re\", 'at', 'still', 'using', 'former', 'an', 'himself', 'made', 'thence', 'another', 'enough', 'only', 'whereas', 'did', 'whom', 'quite', 'indeed', 'during', 'beyond', 'thereby', 'toward', 'its', 'which', 'hence', 'next', 'because', 'namely', 'you', 'is', 'afterwards', 'but', 'front', 'doing', 'wherein', 'well', 'am', 'the', 'moreover', 'yourselves', 'what', 'through', 'whether', 'twelve', 'noone', 'has', 'sometimes', 'out', 'either', 'nobody', 'where', 'perhaps', 'there', 'myself', 'therefore', 'without', 'below', 'bottom', 'anyway', 'this', 'therein', 'those', 'that', 'neither', 'not', 'ever', 'such', 'whence', 'after', 'fifty', 'although', 'few', 'thus', 'themselves', 'up', 'else', 'beforehand', 'otherwise', 'anything', 'off', 'nothing', 'must', 'whereafter', 'top', 'herein', 'nowhere', 'really', 'seems', 'seemed', 'all', 'whereupon', \"'ll\", 'almost', 'full', 'have', 'does', 'empty', 'beside', 'further', 'done', 'even', 'could', 'nor', 'he', 'whenever', 'never', 'ourselves', 'upon', 'they', 'rather', 'first', 'his', 'nine', 'hereupon', 'go', 'just', 'thru', 'often', \"'ve\", 'had', 'between', 'ten', 'nevertheless', 'into', 'however', \"'m\", 'how', 'various', 'anywhere', 'meanwhile', 'already', 'also', 'no', 'onto', 'be', 'who', 'itself', 'somewhere', 'ours', 'are', 'own', 'except', 'while', 'once', 'each', 'alone', 'per', 'others', 'being', 'become', 'yet', 'unless', 'eleven', 'whatever', 'none', 'whither', \"n't\", 'by', 'other', 'elsewhere', 'i', \"'d\", 'then', 'show', 'should', 'formerly', 'our', 'hereby', 'herself', 'many', 'hundred', 'we', 'yours', 'latter', 'above', 'more', 'everyone', 'seeming', 'anyone', 'sixty', 'if', 'whole', 'throughout', 'them', 'much', 'or', 'these', 'under', 'for', 'three', 'along', 'mostly', 'third', 'make', 'something', 'my', 'both', 'say', 'anyhow', 'a', 'your', 'against', 'forty', 'everywhere', 'serious', 'were', 'of', 'whoever', 'him', 're', 'towards', 'some', 'within', 'twenty', 'their', 'very', 'she', 'put', 'so', 'behind', 'than', 'thereupon', 'becomes', 'why', 'when', 'until', 'amongst', 'same', 'as', 'seem', 'four', 'used', 'five', 'among', 'whose', 'six', 'since', 'name', 'mine', 'always', 'became', 'from', 'over', 'move', 'again', 'ca', 'most', 'now', 'me', 'sometime', 'around', 'wherever', 'on', 'yourself', 'see', 'would', 'hers', 'about', 'been', 'several', 'down', 'time', 'trouble', 'notch', 'the', 'ie', 'eg', 'an', 'thing', 'mouth', 'restaurant', 'bit']\n"
     ]
    }
   ],
   "source": [
    "new_stopwords = ['time', 'trouble', 'notch', 'the', 'ie', 'eg', 'an', 'thing', 'mouth', 'restaurant', 'bit']\n",
    "for word in new_stopwords:\n",
    "    stopwords.append(word)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_subj = ['deficiency', 'consistently', 'mouth watering', 'all costs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(sent):\n",
    "    doc = nlp(sent)\n",
    "    \n",
    "    lemmad = \"\"\n",
    "    \n",
    "    #lemmatization\n",
    "    for token in doc:\n",
    "        if token.text not in stopwords:\n",
    "            if token.lemma_ != \"-PRON-\":\n",
    "                temp = token.lemma_.lower().strip()\n",
    "            else:\n",
    "                temp = token.lower_\n",
    "            lemmad = lemmad + temp + \" \"\n",
    "    lemmad = nlp(lemmad)\n",
    "    \n",
    "    #remove stop words and punctuation\n",
    "    cleaned_tokens = \"\"\n",
    "    for token in lemmad:\n",
    "        if token.text not in stopwords and token.text not in punc:\n",
    "            cleaned_tokens = cleaned_tokens + token.text + \" \"\n",
    "    \n",
    "    cleaned_tokens = nlp(cleaned_tokens)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even though its good seafood, the prices are too high.\n",
      "good seafood price high \n"
     ]
    }
   ],
   "source": [
    "sent = \"Even though its good seafood, the prices are too high.\"\n",
    "raw_doc = nlp(sent)\n",
    "print(raw_doc)\n",
    "\n",
    "#pre processed\n",
    "pp_doc = pre_process(sent)\n",
    "print(pp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even \t advmod \t ADV\n",
      "though \t mark \t ADP\n",
      "its \t poss \t DET\n",
      "good \t amod \t ADJ\n",
      "seafood \t nsubj \t NOUN\n",
      ", \t punct \t PUNCT\n",
      "the \t det \t DET\n",
      "prices \t nsubj \t NOUN\n",
      "are \t ROOT \t VERB\n",
      "too \t advmod \t ADV\n",
      "high \t acomp \t ADJ\n",
      ". \t punct \t PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in raw_doc:\n",
    "    print(token.text, '\\t', token.dep_, '\\t', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its good seafood\n",
      "the prices\n",
      "/////\n"
     ]
    }
   ],
   "source": [
    "ents_list = []\n",
    "for ent in raw_doc.ents:\n",
    "    ents_list.append(ent.text)\n",
    "    \n",
    "for chunk in raw_doc.noun_chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "    if chunk.text in ents_list:\n",
    "        print(\"here\")\n",
    "            \n",
    "print(\"/////\")\n",
    "\n",
    "for ent in raw_doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good \t ADJ \t SubClue(rel='weaksubj', pri_pol='positive', stemmed=False)\n",
      "good \t amod \t ADJ\n",
      "seafood \t nsubj \t NOUN\n",
      "prices \t nsubj \t NOUN\n",
      "high \t ADJ \t SubClue(rel='weaksubj', pri_pol='neutral', stemmed=False)\n",
      "high \t acomp \t ADJ\n"
     ]
    }
   ],
   "source": [
    "# printing original token of those in the pre processed document.\n",
    "for token in raw_doc:\n",
    "    if token.lemma_.lower() in pp_doc.text and token.lemma_.lower() != \"the\":\n",
    "        if(subjclues.lookup(token) or token.lemma_.lower() in more_subj):\n",
    "            print(token.text, '\\t', token.pos_, '\\t', subjclues.lookup(token))\n",
    "        \n",
    "        lefts = [t.text for t in token.lefts]\n",
    "        rights = [t.text for t in token.rights]\n",
    "#         print(\"lefts\", lefts)\n",
    "#         print(\"rights\", rights)\n",
    "\n",
    "        printed = False\n",
    "    \n",
    "        if(lefts and rights):\n",
    "            if(lefts[0] == '(' and rights[0] == ')'):\n",
    "                print(lefts[0]+token.text+rights[0], '\\t', token.dep_, '\\t', token.pos_)\n",
    "                printed = True\n",
    "        if not printed:\n",
    "            print(token.text, '\\t', token.dep_, '\\t', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('appetizers', '813'),\n",
       " ('salads', '813'),\n",
       " ('steak', '813'),\n",
       " ('watering', '813'),\n",
       " ('pasta', '813'),\n",
       " ('portions', '1579'),\n",
       " ('lassi', '2882'),\n",
       " ('lamb chettinad', '2882'),\n",
       " ('garlic naan', '2882'),\n",
       " ('rasamalai', '2882'),\n",
       " ('Service', '1609'),\n",
       " ('bills', '3018'),\n",
       " ('Service', '3292'),\n",
       " ('seafood', '3440'),\n",
       " ('prices', '3440'),\n",
       " ('addition', '870'),\n",
       " ('food', '870'),\n",
       " ('prices', '870'),\n",
       " ('food', '2507'),\n",
       " ('garlic shrimp', '3081'),\n",
       " ('okra', '3081'),\n",
       " ('bindi', '3081'),\n",
       " ('lamb', '3081'),\n",
       " ('menu', '1884'),\n",
       " ('selections', '1884'),\n",
       " ('burger', '1884'),\n",
       " ('breads', '2077'),\n",
       " ('food', '201'),\n",
       " ('price', '201'),\n",
       " ('wine list', '201'),\n",
       " ('food', '3049'),\n",
       " ('portions', '3049'),\n",
       " ('coconut rice', '1242'),\n",
       " ('Tom Kha soup', '2211'),\n",
       " ('food', '3343'),\n",
       " ('crunchy tuna', '2141'),\n",
       " ('prices', '83'),\n",
       " ('Sala', '381'),\n",
       " ('service', '3315'),\n",
       " ('food', '3315'),\n",
       " ('price', '3315'),\n",
       " ('beginning', '3315'),\n",
       " ('food', '1709'),\n",
       " ('food', '193'),\n",
       " ('reasons', '2459'),\n",
       " ('calzones', '1293'),\n",
       " ('tips', '2105'),\n",
       " ('turnip cake', '2105'),\n",
       " ('bagels', '2152'),\n",
       " ('ambiance', '882'),\n",
       " ('atmosphere', '882'),\n",
       " ('food', '882'),\n",
       " ('service', '882'),\n",
       " ('waitress', '1124'),\n",
       " ('curry row', '3546'),\n",
       " ('fish', '3020'),\n",
       " ('rice', '3020'),\n",
       " ('seaweed', '3020'),\n",
       " ('woman', '746'),\n",
       " ('spreads', '3170'),\n",
       " ('beverage selections', '3170'),\n",
       " ('bagels', '3170'),\n",
       " ('food options rule', '2171'),\n",
       " ('Japanese Tapas', '2811'),\n",
       " ('Ruby Foo', '762'),\n",
       " ('work', '762'),\n",
       " ('group', '762'),\n",
       " ('sushi', '339'),\n",
       " ('boyfriend', '217'),\n",
       " ('birthday', '217'),\n",
       " ('person', '3478'),\n",
       " ('food', '776'),\n",
       " ('place', '3641'),\n",
       " ('native', '2806'),\n",
       " ('area', '2806'),\n",
       " ('place', '2806'),\n",
       " ('chance', '2630'),\n",
       " ('theater', '924'),\n",
       " ('meal', '924'),\n",
       " ('room', '924'),\n",
       " ('repeat visit', '745'),\n",
       " ('pizza', '1273'),\n",
       " ('Service', '179'),\n",
       " ('OU', '2460'),\n",
       " ('salads', '109'),\n",
       " ('lemon', '109'),\n",
       " ('virgnin olive oil', '109'),\n",
       " ('Drawbacks', '1057'),\n",
       " ('service', '1057'),\n",
       " ('hearty', '1401'),\n",
       " ('find', '1112'),\n",
       " ('mint', '3368'),\n",
       " ('prices', '2451'),\n",
       " ('experience', '2740'),\n",
       " ('food', '1548'),\n",
       " ('all costs', '1548'),\n",
       " ('food', '134'),\n",
       " ('food', '2708'),\n",
       " ('williamsburg', '2591'),\n",
       " ('family', '1593'),\n",
       " ('couple', '3575'),\n",
       " ('door', '3575'),\n",
       " ('waiter', '3575'),\n",
       " ('hurry', '3575'),\n",
       " ('neighborhood spot', '1105'),\n",
       " ('outdoor atmosphere', '742'),\n",
       " ('sidewalk', '742'),\n",
       " ('world', '742'),\n",
       " ('avenue', '742'),\n",
       " ('toppings', '1471'),\n",
       " ('spicy tuna', '1656'),\n",
       " ('salmon', '1656'),\n",
       " ('guaranteeed', '2769'),\n",
       " ('food', '3681'),\n",
       " ('place', '3681'),\n",
       " ('self', '3681'),\n",
       " ('food', '2205'),\n",
       " ('relaxing place', '2205'),\n",
       " ('dinner', '1159'),\n",
       " ('date', '1159'),\n",
       " ('floor', '1159'),\n",
       " ('server', '628'),\n",
       " ('food', '628'),\n",
       " ('dishes', '2912'),\n",
       " ('lamb sausages', '2912'),\n",
       " ('sardines', '2912'),\n",
       " ('biscuits', '2912'),\n",
       " ('shrimp', '2912'),\n",
       " ('pistachio ice cream', '2912'),\n",
       " ('office lunch', '3188'),\n",
       " ('dinner', '3041')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXTRACT ASPECTS\n",
    "my_aspects = []\n",
    "token_aspects = []\n",
    "for i in range(len(text)):\n",
    "    current_sent = text[i][0].text\n",
    "    current_id = text[i][1]\n",
    "    raw_doc = nlp(current_sent)\n",
    "    pp_doc = pre_process(current_sent)\n",
    "    temp_aspect = \"\"\n",
    "    \n",
    "#     list of ents\n",
    "    ents_list = []\n",
    "    for ent in raw_doc.ents:\n",
    "        ents_list.append(ent.text)\n",
    "    \n",
    "    coref_clusters = raw_doc._.coref_clusters\n",
    "\n",
    "    for chunk in raw_doc.noun_chunks:\n",
    "        #do this at start in case next noun chunk is part of the previous one e.g. okra (bindi)\n",
    "        if (chunk.text[0] == \"(\"):\n",
    "            temp_aspect = temp_aspect + chunk.text\n",
    "            temp_aspect += \") \"\n",
    "            previous_i = current_i\n",
    "            current_i = token.i\n",
    "            chunk_span = raw_doc[previous_i:current_i+1]\n",
    "            continue\n",
    "        \n",
    "        if(temp_aspect != ''):\n",
    "            temp_aspect = temp_aspect[:-1]\n",
    "            my_aspects.append((temp_aspect, current_id))\n",
    "            token_aspects.append((chunk_span, current_id))\n",
    "        temp_aspect = \"\"\n",
    "        \n",
    "        if chunk.text in ents_list:\n",
    "            continue\n",
    "        \n",
    "        for cluster in coref_clusters:\n",
    "            if chunk in cluster and chunk != cluster[0]:\n",
    "                continue\n",
    "\n",
    "        #find index of first key aspect word\n",
    "        for token in chunk:\n",
    "            if token.lemma_.lower() in pp_doc.text and not subjclues.lookup(token) and token.lemma_.lower() not in more_subj:\n",
    "                #remove useless words 'the', 'a', etc before a noun. \n",
    "                if((token.lemma_.lower() == \"the\" or token.lemma_.lower() == \"a\") and token.dep_ == \"det\"):\n",
    "                    continue\n",
    "#                 print(token.text)\n",
    "                current_i = token.i\n",
    "                break\n",
    "                \n",
    "        for token in chunk:\n",
    "            #add if its in the pre-processsed doc (meaning its important) and NOT in lexicon.\n",
    "            if token.lemma_.lower() in pp_doc.text and not subjclues.lookup(token) and token.lemma_.lower() not in more_subj:\n",
    "                #remove useless words 'the', 'a', etc before a noun. \n",
    "                if((token.lemma_.lower() == \"the\" or token.lemma_.lower() == \"a\") and token.dep_ == \"det\"):\n",
    "                    continue\n",
    "                if(token.text in ents_list):\n",
    "                    continue\n",
    "#                 print(token.text)\n",
    "                previous_i = current_i\n",
    "                current_i = token.i\n",
    "                temp_aspect = temp_aspect + token.text + \" \"\n",
    "                \n",
    "                chunk_span = raw_doc[previous_i:current_i+1]\n",
    "#         print(\"span\",chunk_span)\n",
    "#         print(\"aspect\",temp_aspect)        \n",
    "    #last time to get the last aspect\n",
    "    if(temp_aspect != ''):\n",
    "            temp_aspect = temp_aspect[:-1]\n",
    "            my_aspects.append((temp_aspect, current_id))\n",
    "            token_aspects.append((chunk_span, current_id))\n",
    "      \n",
    "\n",
    "my_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watering 0.25204514413618084 0.446450245195195\n",
      "\tremoved\n",
      "lassi 0.0946155061908893 0.4269146542649121\n",
      "\tremoved\n",
      "bills 0.18739197509903072 0.573517877350986\n",
      "\tremoved\n",
      "addition 0.2438230634555494 0.4986083795451099\n",
      "\tremoved\n",
      "bindi -0.0006434869015635305 0.31746877948368774\n",
      "\tremoved\n",
      "menu 0.28032961376081605 0.5310790559565972\n",
      "\tremoved\n",
      "Sala 0.044741066006610014 0.2611578706321381\n",
      "\tremoved\n",
      "beginning 0.20776557813534097 0.5355942979744079\n",
      "\tremoved\n",
      "reasons 0.23896812079279373 0.596764846284457\n",
      "\tremoved\n",
      "calzones 0.12648731817442488 0.5383531232314991\n",
      "\tremoved\n",
      "tips 0.20676653576281945 0.4424892874515434\n",
      "\tremoved\n",
      "woman 0.1975070254490384 0.5618484262173992\n",
      "\tremoved\n",
      "spreads 0.15812113426369453 0.3035282482031155\n",
      "\tremoved\n",
      "Ruby Foo 0.13982881779427336 0.4080430243701792\n",
      "\tremoved\n",
      "work 0.23067486292963418 0.5661829527584181\n",
      "\tremoved\n",
      "group 0.15871121658826423 0.4163860281580687\n",
      "\tremoved\n",
      "boyfriend 0.18269706751717343 0.5278435031266772\n",
      "\tremoved\n",
      "birthday 0.2112237437685425 0.5475895588717791\n",
      "\tremoved\n",
      "person 0.2393872318425593 0.5618484262173992\n",
      "\tremoved\n",
      "chance 0.2285634298767699 0.5664582037143493\n",
      "\tremoved\n",
      "theater 0.1799340451132339 0.3893649760090883\n",
      "\tremoved\n",
      "repeat visit 0.23087048258252296 0.5448044113383214\n",
      "\tremoved\n",
      "OU 0.06873818702360421 0.21341665610642752\n",
      "\tremoved\n",
      "Drawbacks 0.11449867750627654 0.5203752100596121\n",
      "\tremoved\n",
      "find 0.26985700506847016 0.5935838201250794\n",
      "\tremoved\n",
      "experience 0.23430346213438769 0.5661829527584181\n",
      "\tremoved\n",
      "all costs 0.2915612792067321 0.596764846284457\n",
      "\tremoved\n",
      "williamsburg 0.0798477266394054 0.2787150930899724\n",
      "\tremoved\n",
      "family 0.2474642699537283 0.46666182125059513\n",
      "\tremoved\n",
      "couple 0.2645156524446494 0.5264959991304593\n",
      "\tremoved\n",
      "door 0.19069606866514296 0.5904292518027869\n",
      "\tremoved\n",
      "hurry 0.21905147998386554 0.5093597891252468\n",
      "\tremoved\n",
      "sidewalk 0.1904460309196978 0.5508023579323125\n",
      "\tremoved\n",
      "world 0.217130521515567 0.5575021523167933\n",
      "\tremoved\n",
      "self 0.1804909545845392 0.4988018373242008\n",
      "\tremoved\n",
      "date 0.1592265572465508 0.47887749141806857\n",
      "\tremoved\n",
      "server 0.14144396730095724 0.44834054631561465\n",
      "\tremoved\n"
     ]
    }
   ],
   "source": [
    "for current_token in token_aspects:\n",
    "    max_sim = 0\n",
    "    sum_sim = 0\n",
    "    avg_sim = 0\n",
    "    for compare_token in token_aspects:\n",
    "        if current_token[1] != compare_token[1]:\n",
    "            if(current_token[0].similarity(compare_token[0])):\n",
    "                sim_score = current_token[0].similarity(compare_token[0])\n",
    "                sum_sim += sim_score\n",
    "                if max_sim < sim_score:\n",
    "                    max_sim = sim_score\n",
    "#     print(current_token[0], avg_sim, max_sim)\n",
    "    avg_sim = sum_sim / len(token_aspects)\n",
    "    if (avg_sim < 0.17 or max_sim < 0.60) and avg_sim != 0.0:\n",
    "#     if avg_sim < 0.30 and max_sim < 0.53:\n",
    "        if((current_token[0].text, current_token[1]) in my_aspects):\n",
    "            my_aspects.remove((current_token[0].text, current_token[1]))\n",
    "            print(current_token[0], avg_sim, max_sim)\n",
    "            print(\"\\tremoved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('appetizers', '813'),\n",
       " ('salads', '813'),\n",
       " ('steak', '813'),\n",
       " ('pasta', '813'),\n",
       " ('portions', '1579'),\n",
       " ('lamb chettinad', '2882'),\n",
       " ('garlic naan', '2882'),\n",
       " ('rasamalai', '2882'),\n",
       " ('Service', '1609'),\n",
       " ('Service', '3292'),\n",
       " ('seafood', '3440'),\n",
       " ('prices', '3440'),\n",
       " ('food', '870'),\n",
       " ('prices', '870'),\n",
       " ('food', '2507'),\n",
       " ('garlic shrimp', '3081'),\n",
       " ('okra', '3081'),\n",
       " ('lamb', '3081'),\n",
       " ('selections', '1884'),\n",
       " ('burger', '1884'),\n",
       " ('breads', '2077'),\n",
       " ('food', '201'),\n",
       " ('price', '201'),\n",
       " ('wine list', '201'),\n",
       " ('food', '3049'),\n",
       " ('portions', '3049'),\n",
       " ('coconut rice', '1242'),\n",
       " ('Tom Kha soup', '2211'),\n",
       " ('food', '3343'),\n",
       " ('crunchy tuna', '2141'),\n",
       " ('prices', '83'),\n",
       " ('service', '3315'),\n",
       " ('food', '3315'),\n",
       " ('price', '3315'),\n",
       " ('food', '1709'),\n",
       " ('food', '193'),\n",
       " ('turnip cake', '2105'),\n",
       " ('bagels', '2152'),\n",
       " ('ambiance', '882'),\n",
       " ('atmosphere', '882'),\n",
       " ('food', '882'),\n",
       " ('service', '882'),\n",
       " ('waitress', '1124'),\n",
       " ('curry row', '3546'),\n",
       " ('fish', '3020'),\n",
       " ('rice', '3020'),\n",
       " ('seaweed', '3020'),\n",
       " ('beverage selections', '3170'),\n",
       " ('bagels', '3170'),\n",
       " ('food options rule', '2171'),\n",
       " ('Japanese Tapas', '2811'),\n",
       " ('sushi', '339'),\n",
       " ('food', '776'),\n",
       " ('place', '3641'),\n",
       " ('native', '2806'),\n",
       " ('area', '2806'),\n",
       " ('place', '2806'),\n",
       " ('meal', '924'),\n",
       " ('room', '924'),\n",
       " ('pizza', '1273'),\n",
       " ('Service', '179'),\n",
       " ('salads', '109'),\n",
       " ('lemon', '109'),\n",
       " ('virgnin olive oil', '109'),\n",
       " ('service', '1057'),\n",
       " ('hearty', '1401'),\n",
       " ('mint', '3368'),\n",
       " ('prices', '2451'),\n",
       " ('food', '1548'),\n",
       " ('food', '134'),\n",
       " ('food', '2708'),\n",
       " ('waiter', '3575'),\n",
       " ('neighborhood spot', '1105'),\n",
       " ('outdoor atmosphere', '742'),\n",
       " ('avenue', '742'),\n",
       " ('toppings', '1471'),\n",
       " ('spicy tuna', '1656'),\n",
       " ('salmon', '1656'),\n",
       " ('guaranteeed', '2769'),\n",
       " ('food', '3681'),\n",
       " ('place', '3681'),\n",
       " ('food', '2205'),\n",
       " ('relaxing place', '2205'),\n",
       " ('dinner', '1159'),\n",
       " ('floor', '1159'),\n",
       " ('food', '628'),\n",
       " ('dishes', '2912'),\n",
       " ('lamb sausages', '2912'),\n",
       " ('sardines', '2912'),\n",
       " ('biscuits', '2912'),\n",
       " ('shrimp', '2912'),\n",
       " ('pistachio ice cream', '2912'),\n",
       " ('office lunch', '3188'),\n",
       " ('dinner', '3041')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('appetizers', '813'),\n",
       " ('salads', '813'),\n",
       " ('steak', '813'),\n",
       " ('pasta', '813'),\n",
       " ('portions', '1579'),\n",
       " ('sweet lassi', '2882'),\n",
       " ('lamb chettinad', '2882'),\n",
       " ('garlic naan', '2882'),\n",
       " ('rasamalai', '2882'),\n",
       " ('Service', '1609'),\n",
       " ('bills', '3018'),\n",
       " ('Service', '3292'),\n",
       " ('lambc hops', '2041'),\n",
       " ('seafood', '3440'),\n",
       " ('prices', '3440'),\n",
       " ('food', '870'),\n",
       " ('prices', '870'),\n",
       " ('food', '2507'),\n",
       " ('garlic shrimp', '3081'),\n",
       " ('okra (bindi)', '3081'),\n",
       " ('lamb', '3081'),\n",
       " ('menu', '1884'),\n",
       " ('burger', '1884'),\n",
       " ('steak', '1884'),\n",
       " ('escargot', '1884'),\n",
       " ('breads', '2077'),\n",
       " ('food', '201'),\n",
       " ('price', '201'),\n",
       " ('wine list', '201'),\n",
       " ('food', '3049'),\n",
       " ('portions', '3049'),\n",
       " ('coconut rice', '1242'),\n",
       " ('Tom Kha soup', '2211'),\n",
       " ('food', '3343'),\n",
       " ('crunchy tuna', '2141'),\n",
       " ('prices', '83'),\n",
       " ('service', '3315'),\n",
       " ('food', '3315'),\n",
       " ('price', '3315'),\n",
       " ('food', '1709'),\n",
       " ('food', '193'),\n",
       " ('priced', '193'),\n",
       " ('calzones', '1293'),\n",
       " ('turnip cake', '2105'),\n",
       " ('roast pork buns', '2105'),\n",
       " ('egg custards', '2105'),\n",
       " ('bagels', '2152'),\n",
       " ('ambiance', '882'),\n",
       " ('atmosphere', '882'),\n",
       " ('food', '882'),\n",
       " ('service', '882'),\n",
       " ('waitress', '1124'),\n",
       " ('fish', '3020'),\n",
       " ('rice', '3020'),\n",
       " ('seaweed', '3020'),\n",
       " ('spreads', '3170'),\n",
       " ('beverage selections', '3170'),\n",
       " ('bagels', '3170'),\n",
       " ('food options', '2171'),\n",
       " ('Japanese Tapas', '2811'),\n",
       " ('sushi', '339'),\n",
       " ('food', '776'),\n",
       " ('meal', '924'),\n",
       " ('room', '924'),\n",
       " ('pizza', '1273'),\n",
       " ('Service', '179'),\n",
       " ('lamb chop', '109'),\n",
       " ('salads', '109'),\n",
       " ('service', '1057'),\n",
       " ('prices', '2451'),\n",
       " ('food', '1548'),\n",
       " ('food', '134'),\n",
       " ('food', '2708'),\n",
       " ('priced', '2708'),\n",
       " ('waiter', '3575'),\n",
       " ('outdoor atmosphere', '742'),\n",
       " ('toppings', '1471'),\n",
       " ('spicy tuna', '1656'),\n",
       " ('salmon', '1656'),\n",
       " ('food', '3681'),\n",
       " ('place', '3681'),\n",
       " ('food', '2205'),\n",
       " ('place', '2205'),\n",
       " ('dinner', '1159'),\n",
       " ('dance floor', '1159'),\n",
       " ('server', '628'),\n",
       " ('served', '628'),\n",
       " ('food', '628'),\n",
       " ('drinks', '628'),\n",
       " ('dishes', '2912'),\n",
       " ('lamb sausages', '2912'),\n",
       " ('sardines with biscuits', '2912'),\n",
       " ('large whole shrimp', '2912'),\n",
       " ('pistachio ice cream', '2912'),\n",
       " ('office lunch', '3188'),\n",
       " ('dinner', '3041')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the true aspects for each sentence\n",
    "true_aspects = []\n",
    "for i in range(len(aspects)):\n",
    "    true_aspects.append((aspects[i][0].term, aspects[i][1]))\n",
    "true_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision  0.7872340425531915\n",
      "recall 0.7708333333333334\n",
      "f_score 0.7789473684210526\n"
     ]
    }
   ],
   "source": [
    "#compare\n",
    "total_true = len(true_aspects)\n",
    "total_returned = len(my_aspects)\n",
    "\n",
    "correct = 0\n",
    "for aspect in my_aspects:\n",
    "    if aspect in true_aspects:\n",
    "        correct = correct + 1\n",
    "#     else:\n",
    "#         correct -= 1\n",
    "\n",
    "# percent = (correct / total_true) * 100\n",
    "# print(percent, \"%\")\n",
    "\n",
    "precision = correct /total_returned \n",
    "print(\"precision \", precision)\n",
    "\n",
    "recall = correct / total_true\n",
    "print(\"recall\", recall)\n",
    "\n",
    "f_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"f_score\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine rule based method\n",
    "#can never achieve full coverage with rule based\n",
    "#not generalised method - cant continually add rules\n",
    "#is there any way to generalise \n",
    "#look at papers, research new ways to combine with rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_aspects)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'a') as f:\n",
    "    for i in range(len(my_aspects)):\n",
    "        print(my_aspects[i], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"Scientists know many things about the Sun. They know how old it is. The Sun is more than 4½ billion years old. It is also a star that is the centre of our solar system. They also know the Sun’s size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists: [Scientists, They, They]\n",
      "the Sun: [the Sun, The Sun, Sun, It, Sun]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "the Sun"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_clusters = doc1._.coref_clusters\n",
    "print(coref_clusters[0])\n",
    "# print(coref_clusters[0][1])\n",
    "\n",
    "print(coref_clusters[1])\n",
    "# print(coref_clusters[1][2])\n",
    "\n",
    "coref_clusters[1].mentions[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists\n",
      "the Sun\n",
      "They\n",
      "\tremoved\n",
      "\tremoved\n",
      "The Sun\n",
      "\tremoved\n",
      "It\n",
      "\tremoved\n",
      "\tremoved\n",
      "They\n",
      "\tremoved\n",
      "\tremoved\n"
     ]
    }
   ],
   "source": [
    "pronoun_list=['he','she','it','they','them','him','her','his','hers','its','we','us']\n",
    "# for chunk in doc1.noun_chunks:\n",
    "#     print(chunk)\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "for chunk in doc1.noun_chunks:\n",
    "    for cluster in coref_clusters:\n",
    "#         print(cluster)\n",
    "#         print(cur_span)\n",
    "        if chunk in cluster:\n",
    "            print(chunk)\n",
    "\n",
    "#             if(chunk.text.lower() in pronoun_list):\n",
    "#                 print(\"\\tremoved\")\n",
    "            \n",
    "            if(chunk != cluster[0]):\n",
    "                print('\\tremoved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
